{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Training, Validation, and Test/Prediction Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "stop_words =  set(stopwords.words('english'))\n",
    "trainings_df = pd.read_table(\"train.tsv\") #156060\n",
    "test_df = pd.read_table(\"test.tsv\") #66292\n",
    "\n",
    "trainings_df = trainings_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df = trainings_df[:135000]\n",
    "val_df = trainings_df[135000:]\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "val_df = val_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_df), len(train_df), len(val_df))\n",
    "print(test_df.keys(), train_df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    \n",
    "    def __init__(self, df, reduction):\n",
    "        embeddings, labels = get_data(df)\n",
    "        reduced_embeddings = reduction.fit_transform(embeddings)\n",
    "    \n",
    "        self.embeddings = reduced_embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods for processing the sentences into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence(sentence):\n",
    "    tokens = gensim.utils.simple_preprocess(sentence)#nltk.tokenize.word_tokenize(sentence)\n",
    "    filtered = [w.lower() for w in tokens if not w in stop_words and w in word_model.vocab]\n",
    "    return filtered\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    vectors = np.zeros((len(sentence),300))\n",
    "    for i, word in enumerate(sentence):\n",
    "        embedding = word_model.wv[word]\n",
    "        assert np.all(np.isfinite(embedding))\n",
    "        vectors[i] = embedding\n",
    "    mean = np.mean(vectors, axis=0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def get_data(df):\n",
    "    \n",
    "    #nn.CrossEntropy doesn't used one_hot encoded\n",
    "    # label_source = np.eye(5)\n",
    "\n",
    "    all_sentences = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        filtered = filter_sentence(row['Phrase'])\n",
    "        if not len(filtered)==0:\n",
    "            all_sentences.append( (filtered, row['Sentiment']))\n",
    "    \n",
    "    num_data = len(all_sentences)\n",
    "    # tuples of (embedding, label)\n",
    "    embeddings = np.zeros( (num_data, 300))\n",
    "    # labels = np.zeros((num_data, 5)) instead of using one_hot\n",
    "    labels = np.zeros((num_data))\n",
    "    for i, (sentence, sentiment) in enumerate(all_sentences):\n",
    "        embedding = get_sentence_embedding(sentence)\n",
    "        labels[i] = sentiment    \n",
    "        embeddings[i] = embedding\n",
    "    \n",
    "    # normalize the data a bit, needed for PCA\n",
    "    embeddings -= np.mean(embeddings)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, valloader, epochs=2, lr=1e-3, regularization=1e-4, print_every=2):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=regularization)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        if epoch % print_every==0:\n",
    "            test(model, trainloader, name=\"train\")\n",
    "            test(model, valloader)\n",
    "\n",
    "    print('Finished Training')\n",
    "    test(model, valloader)\n",
    "    return model\n",
    "\n",
    "def test(model, valloader, name=\"val\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            \n",
    "            images, labels = data\n",
    "            images = images.float()\n",
    "            labels = labels.long()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the %s %% images: %d %%' % (name,\n",
    "        100 * correct / total))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_df, val_df, reduction=PCA(n_components=20), batch_size=4):\n",
    "    trainloader = DataLoader(SentimentData(train_df, reduction), batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(SentimentData(val_df, reduction), batch_size=batch_size, shuffle=True)\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 200\n",
    "output_size = 5\n",
    "batch_size = 16\n",
    "t, v = get_loaders(train_df, val_df, batch_size=batch_size, reduction=PCA(n_components=input_size))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 1, -1)\n",
    "\n",
    "hidden_size = [256, 128, 128, 256]\n",
    "channels = [1, 32, 64, 32]\n",
    "complicated = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size[0] ),\n",
    "        nn.ReLU(),\n",
    "        UnFlatten(),\n",
    "        nn.Conv1d(channels[0], channels[1], kernel_size=3, padding=1, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.MaxPool1d(kernel_size=2),\n",
    "        nn.Conv1d(channels[1], channels[2], kernel_size=5, padding=2, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.MaxPool1d(kernel_size=2),\n",
    "        nn.Conv1d(channels[2], channels[3], kernel_size=3, padding=1, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.MaxPool1d(kernel_size=2),\n",
    "        Flatten(),\n",
    "        nn.Linear(channels[3] / 8 * hidden_size[0], output_size),\n",
    "        \n",
    ")  \n",
    "\n",
    "simple = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size[2], hidden_size[3]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size[3], output_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train(complicated, t, v,regularization=5e-3, lr=1e-3, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flower]",
   "language": "python",
   "name": "conda-env-flower-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
